{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b385ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from scipy.ndimage import zoom, binary_erosion, binary_dilation\n",
    "from scipy.ndimage.measurements import label, center_of_mass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e698c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PancreasDataPipeline:\n",
    "    \"\"\"\n",
    "    Complete data pipeline for pancreatic tumor CLDM training\n",
    "    Handles MSD dataset preprocessing, size classification, and train/val/test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, output_dir, target_spacing=(1.0, 1.0, 1.0), \n",
    "                 target_size=(128, 128, 64), hu_window=(-100, 240)):\n",
    "        \"\"\"\n",
    "        Initialize the data pipeline\n",
    "        \n",
    "        Args:\n",
    "            data_root: Path to MSD Task07_Pancreas dataset\n",
    "            output_dir: Where to save processed data\n",
    "            target_spacing: Target voxel spacing in mm (x, y, z)\n",
    "            target_size: Target volume dimensions (H, W, D)\n",
    "            hu_window: HU windowing range for CT normalization\n",
    "        \"\"\"\n",
    "        self.data_root = Path(data_root)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.target_spacing = target_spacing\n",
    "        self.target_size = target_size\n",
    "        self.hu_window = hu_window\n",
    "        \n",
    "        # Size thresholds based on your analysis\n",
    "        self.size_thresholds = {\n",
    "            'small_max': 20000,    # <20k mm³\n",
    "            'medium_max': 125000   # 20k-125k mm³, >125k = large\n",
    "        }\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (self.output_dir / 'processed_volumes').mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'processed_masks').mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'metadata').mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'splits').mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"🏥 Pancreatic Tumor CLDM Pipeline Initialized\")\n",
    "        print(f\"📁 Data root: {self.data_root}\")\n",
    "        print(f\"💾 Output: {self.output_dir}\")\n",
    "        print(f\"🎯 Target size: {self.target_size}\")\n",
    "        print(f\"📏 Target spacing: {self.target_spacing}\")\n",
    "    \n",
    "    def load_msd_data(self):\n",
    "        \"\"\"Load and parse MSD dataset structure\"\"\"\n",
    "        \n",
    "        # Load dataset JSON\n",
    "        dataset_json = self.data_root / 'dataset.json'\n",
    "        if not dataset_json.exists():\n",
    "            raise FileNotFoundError(f\"Dataset JSON not found: {dataset_json}\")\n",
    "            \n",
    "        with open(dataset_json, 'r') as f:\n",
    "            dataset_info = json.load(f)\n",
    "        \n",
    "        print(f\"📋 Dataset: {dataset_info['name']}\")\n",
    "        print(f\"📝 Description: {dataset_info['description']}\")\n",
    "        \n",
    "        # Get training files\n",
    "        train_files = []\n",
    "        images_dir = self.data_root / 'imagesTr'\n",
    "        labels_dir = self.data_root / 'labelsTr'\n",
    "        \n",
    "        for item in dataset_info['training']:\n",
    "            image_path = images_dir / item['image'].split('/')[-1]\n",
    "            label_path = labels_dir / item['label'].split('/')[-1]\n",
    "            \n",
    "            if image_path.exists() and label_path.exists():\n",
    "                train_files.append({\n",
    "                    'image': str(image_path),\n",
    "                    'label': str(label_path),\n",
    "                    'case_id': image_path.stem\n",
    "                })\n",
    "        \n",
    "        print(f\"✅ Found {len(train_files)} training cases\")\n",
    "        return train_files, dataset_info\n",
    "    \n",
    "    def normalize_ct_volume(self, volume):\n",
    "        \"\"\"\n",
    "        Normalize CT volume with HU windowing and z-score normalization\n",
    "        \n",
    "        Args:\n",
    "            volume: Raw CT volume array\n",
    "            \n",
    "        Returns:\n",
    "            Normalized volume array\n",
    "        \"\"\"\n",
    "        # HU windowing\n",
    "        volume = np.clip(volume, self.hu_window[0], self.hu_window[1])\n",
    "        \n",
    "        # Z-score normalization within windowed range\n",
    "        mean_val = np.mean(volume)\n",
    "        std_val = np.std(volume)\n",
    "        if std_val > 0:\n",
    "            volume = (volume - mean_val) / std_val\n",
    "        \n",
    "        return volume.astype(np.float32)\n",
    "    \n",
    "    def resample_volume(self, volume, original_spacing, target_spacing):\n",
    "        \"\"\"\n",
    "        Resample volume to target spacing\n",
    "        \n",
    "        Args:\n",
    "            volume: Input volume array\n",
    "            original_spacing: Original voxel spacing (x, y, z)\n",
    "            target_spacing: Target voxel spacing (x, y, z)\n",
    "            \n",
    "        Returns:\n",
    "            Resampled volume\n",
    "        \"\"\"\n",
    "        # Calculate zoom factors\n",
    "        zoom_factors = [\n",
    "            orig / target for orig, target in zip(original_spacing, target_spacing)\n",
    "        ]\n",
    "        \n",
    "        # Resample using trilinear interpolation\n",
    "        resampled = zoom(volume, zoom_factors, order=1, mode='constant', cval=0)\n",
    "        \n",
    "        return resampled\n",
    "    \n",
    "    def extract_pancreas_region(self, volume, pancreas_mask, tumor_mask, margin=10):\n",
    "        \"\"\"\n",
    "        Extract pancreas-centered region with margin\n",
    "        \n",
    "        Args:\n",
    "            volume: CT volume\n",
    "            pancreas_mask: Pancreas segmentation mask\n",
    "            tumor_mask: Tumor segmentation mask\n",
    "            margin: Additional margin around pancreas (voxels)\n",
    "            \n",
    "        Returns:\n",
    "            Cropped volume, pancreas_mask, tumor_mask, bounding_box\n",
    "        \"\"\"\n",
    "        # Find pancreas bounding box\n",
    "        coords = np.where(pancreas_mask > 0)\n",
    "        if len(coords[0]) == 0:\n",
    "            raise ValueError(\"No pancreas found in mask\")\n",
    "        \n",
    "        min_coords = [np.min(coord) for coord in coords]\n",
    "        max_coords = [np.max(coord) for coord in coords]\n",
    "        \n",
    "        # Add margin\n",
    "        bbox = []\n",
    "        for i, (min_c, max_c) in enumerate(zip(min_coords, max_coords)):\n",
    "            start = max(0, min_c - margin)\n",
    "            end = min(volume.shape[i], max_c + margin + 1)\n",
    "            bbox.append((start, end))\n",
    "        \n",
    "        # Extract regions\n",
    "        cropped_volume = volume[\n",
    "            bbox[0][0]:bbox[0][1],\n",
    "            bbox[1][0]:bbox[1][1], \n",
    "            bbox[2][0]:bbox[2][1]\n",
    "        ]\n",
    "        cropped_pancreas = pancreas_mask[\n",
    "            bbox[0][0]:bbox[0][1],\n",
    "            bbox[1][0]:bbox[1][1],\n",
    "            bbox[2][0]:bbox[2][1]\n",
    "        ]\n",
    "        cropped_tumor = tumor_mask[\n",
    "            bbox[0][0]:bbox[0][1],\n",
    "            bbox[1][0]:bbox[1][1],\n",
    "            bbox[2][0]:bbox[2][1]\n",
    "        ]\n",
    "        \n",
    "        return cropped_volume, cropped_pancreas, cropped_tumor, bbox\n",
    "    \n",
    "    def resize_to_target(self, volume, target_size):\n",
    "        \"\"\"\n",
    "        Resize volume to target dimensions\n",
    "        \n",
    "        Args:\n",
    "            volume: Input volume\n",
    "            target_size: Target dimensions (H, W, D)\n",
    "            \n",
    "        Returns:\n",
    "            Resized volume\n",
    "        \"\"\"\n",
    "        zoom_factors = [\n",
    "            target / current for target, current in zip(target_size, volume.shape)\n",
    "        ]\n",
    "        \n",
    "        return zoom(volume, zoom_factors, order=1, mode='constant', cval=0)\n",
    "    \n",
    "    def calculate_tumor_metrics(self, tumor_mask, spacing):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive tumor metrics\n",
    "        \n",
    "        Args:\n",
    "            tumor_mask: Binary tumor mask\n",
    "            spacing: Voxel spacing (x, y, z)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of tumor metrics\n",
    "        \"\"\"\n",
    "        if np.sum(tumor_mask) == 0:\n",
    "            return {\n",
    "                'volume_mm3': 0,\n",
    "                'volume_voxels': 0,\n",
    "                'centroid': (0, 0, 0),\n",
    "                'bounding_box': ((0, 0), (0, 0), (0, 0)),\n",
    "                'size_class': 'none'\n",
    "            }\n",
    "        \n",
    "        # Volume calculation\n",
    "        voxel_volume = np.prod(spacing)  # mm³ per voxel\n",
    "        volume_voxels = np.sum(tumor_mask)\n",
    "        volume_mm3 = volume_voxels * voxel_volume\n",
    "        \n",
    "        # Size classification based on your thresholds\n",
    "        if volume_mm3 < self.size_thresholds['small_max']:\n",
    "            size_class = 'small'\n",
    "        elif volume_mm3 < self.size_thresholds['medium_max']:\n",
    "            size_class = 'medium'\n",
    "        else:\n",
    "            size_class = 'large'\n",
    "        \n",
    "        # Centroid calculation\n",
    "        centroid = center_of_mass(tumor_mask)\n",
    "        \n",
    "        # Bounding box\n",
    "        coords = np.where(tumor_mask > 0)\n",
    "        bbox = tuple((np.min(coord), np.max(coord)) for coord in coords)\n",
    "        \n",
    "        return {\n",
    "            'volume_mm3': float(volume_mm3),\n",
    "            'volume_voxels': int(volume_voxels),\n",
    "            'centroid': centroid,\n",
    "            'bounding_box': bbox,\n",
    "            'size_class': size_class\n",
    "        }\n",
    "    \n",
    "    def process_single_case(self, case_info):\n",
    "        \"\"\"\n",
    "        Process a single case through the complete pipeline\n",
    "        \n",
    "        Args:\n",
    "            case_info: Dictionary with image/label paths and case_id\n",
    "            \n",
    "        Returns:\n",
    "            Processed data dictionary or None if processing fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            case_id = case_info['case_id']\n",
    "            print(f\"🔄 Processing {case_id}...\")\n",
    "            \n",
    "            # Load NIfTI files\n",
    "            img_nii = nib.load(case_info['image'])\n",
    "            label_nii = nib.load(case_info['label'])\n",
    "            \n",
    "            volume = img_nii.get_fdata()\n",
    "            labels = label_nii.get_fdata()\n",
    "            original_spacing = img_nii.header.get_zooms()[:3]\n",
    "            \n",
    "            # Extract masks (MSD labels: 1=pancreas, 2=tumor)\n",
    "            pancreas_mask = (labels == 1).astype(np.uint8)\n",
    "            tumor_mask = (labels == 2).astype(np.uint8)\n",
    "            \n",
    "            # Calculate original tumor metrics\n",
    "            tumor_metrics = self.calculate_tumor_metrics(tumor_mask, original_spacing)\n",
    "            \n",
    "            # Skip cases without tumors\n",
    "            if tumor_metrics['volume_mm3'] == 0:\n",
    "                print(f\"⚠️  {case_id}: No tumor found, skipping\")\n",
    "                return None\n",
    "            \n",
    "            # Resample to target spacing\n",
    "            volume_resampled = self.resample_volume(volume, original_spacing, self.target_spacing)\n",
    "            pancreas_resampled = self.resample_volume(pancreas_mask.astype(float), \n",
    "                                                    original_spacing, self.target_spacing)\n",
    "            tumor_resampled = self.resample_volume(tumor_mask.astype(float), \n",
    "                                                 original_spacing, self.target_spacing)\n",
    "            \n",
    "            # Convert back to binary masks\n",
    "            pancreas_resampled = (pancreas_resampled > 0.5).astype(np.uint8)\n",
    "            tumor_resampled = (tumor_resampled > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Recalculate tumor metrics after resampling\n",
    "            tumor_metrics_resampled = self.calculate_tumor_metrics(tumor_resampled, self.target_spacing)\n",
    "            \n",
    "            # Extract pancreas-centered region\n",
    "            cropped_volume, cropped_pancreas, cropped_tumor, bbox = self.extract_pancreas_region(\n",
    "                volume_resampled, pancreas_resampled, tumor_resampled\n",
    "            )\n",
    "            \n",
    "            # Normalize CT values\n",
    "            normalized_volume = self.normalize_ct_volume(cropped_volume)\n",
    "            \n",
    "            # Resize to target dimensions\n",
    "            final_volume = self.resize_to_target(normalized_volume, self.target_size)\n",
    "            final_pancreas = self.resize_to_target(cropped_pancreas.astype(float), self.target_size)\n",
    "            final_tumor = self.resize_to_target(cropped_tumor.astype(float), self.target_size)\n",
    "            \n",
    "            # Convert back to binary masks\n",
    "            final_pancreas = (final_pancreas > 0.5).astype(np.uint8)\n",
    "            final_tumor = (final_tumor > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Save processed volumes\n",
    "            np.save(self.output_dir / 'processed_volumes' / f'{case_id}_volume.npy', final_volume)\n",
    "            np.save(self.output_dir / 'processed_masks' / f'{case_id}_pancreas.npy', final_pancreas)\n",
    "            np.save(self.output_dir / 'processed_masks' / f'{case_id}_tumor.npy', final_tumor)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = {\n",
    "                'case_id': case_id,\n",
    "                'original_shape': volume.shape,\n",
    "                'processed_shape': final_volume.shape,\n",
    "                'original_spacing': original_spacing,\n",
    "                'target_spacing': self.target_spacing,\n",
    "                'tumor_volume_mm3': tumor_metrics_resampled['volume_mm3'],\n",
    "                'size_class': tumor_metrics_resampled['size_class'],\n",
    "                'tumor_centroid': tumor_metrics_resampled['centroid'],\n",
    "                'bounding_box': bbox,\n",
    "                'has_tumor': True,\n",
    "                'processing_successful': True\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {case_id}: {tumor_metrics_resampled['size_class']} tumor \"\n",
    "                  f\"({tumor_metrics_resampled['volume_mm3']:.0f} mm³)\")\n",
    "            \n",
    "            return metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {case_id}: Processing failed - {str(e)}\")\n",
    "            return {\n",
    "                'case_id': case_id,\n",
    "                'processing_successful': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def create_stratified_splits(self, metadata_df, test_size=0.2, val_size=0.15, random_state=42):\n",
    "        \"\"\"\n",
    "        Create stratified train/validation/test splits maintaining class ratios\n",
    "        \n",
    "        Args:\n",
    "            metadata_df: DataFrame with case metadata\n",
    "            test_size: Proportion for test set\n",
    "            val_size: Proportion for validation set (from remaining after test)\n",
    "            random_state: Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with train/val/test case IDs\n",
    "        \"\"\"\n",
    "        # Filter successful cases with tumors\n",
    "        valid_cases = metadata_df[\n",
    "            (metadata_df['processing_successful'] == True) & \n",
    "            (metadata_df['has_tumor'] == True)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"\\n📊 Creating stratified splits from {len(valid_cases)} valid cases\")\n",
    "        \n",
    "        # Display class distribution\n",
    "        class_counts = valid_cases['size_class'].value_counts()\n",
    "        print(\"Class distribution:\")\n",
    "        for size_class, count in class_counts.items():\n",
    "            print(f\"  {size_class}: {count} cases\")\n",
    "        \n",
    "        # Handle edge case for large tumors (only 4 cases)\n",
    "        if class_counts.get('large', 0) <= 2:\n",
    "            print(\"⚠️  Warning: Very few large cases. Adjusting split strategy...\")\n",
    "            \n",
    "            # Separate large cases for special handling\n",
    "            large_cases = valid_cases[valid_cases['size_class'] == 'large']\n",
    "            other_cases = valid_cases[valid_cases['size_class'] != 'large']\n",
    "            \n",
    "            # Split large cases manually (keep at least 1 for each split)\n",
    "            large_ids = large_cases['case_id'].tolist()\n",
    "            if len(large_ids) >= 3:\n",
    "                large_train = large_ids[:-2]\n",
    "                large_val = [large_ids[-2]]\n",
    "                large_test = [large_ids[-1]]\n",
    "            elif len(large_ids) == 2:\n",
    "                large_train = [large_ids[0]]\n",
    "                large_val = []\n",
    "                large_test = [large_ids[1]]\n",
    "            else:\n",
    "                large_train = large_ids\n",
    "                large_val = []\n",
    "                large_test = []\n",
    "            \n",
    "            # Split other cases normally\n",
    "            if len(other_cases) > 0:\n",
    "                X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "                    other_cases['case_id'].values,\n",
    "                    other_cases['size_class'].values,\n",
    "                    test_size=test_size,\n",
    "                    stratify=other_cases['size_class'].values,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "                \n",
    "                X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X_temp, y_temp,\n",
    "                    test_size=val_size,\n",
    "                    stratify=y_temp,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "                \n",
    "                # Combine with large cases\n",
    "                train_ids = list(X_train) + large_train\n",
    "                val_ids = list(X_val) + large_val\n",
    "                test_ids = list(X_test) + large_test\n",
    "            else:\n",
    "                train_ids = large_train\n",
    "                val_ids = large_val\n",
    "                test_ids = large_test\n",
    "        \n",
    "        else:\n",
    "            # Standard stratified split for balanced classes\n",
    "            X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "                valid_cases['case_id'].values,\n",
    "                valid_cases['size_class'].values,\n",
    "                test_size=test_size,\n",
    "                stratify=valid_cases['size_class'].values,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_temp, y_temp,\n",
    "                test_size=val_size,\n",
    "                stratify=y_temp,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            train_ids = list(X_train)\n",
    "            val_ids = list(X_val)\n",
    "            test_ids = list(X_test)\n",
    "        \n",
    "        splits = {\n",
    "            'train': train_ids,\n",
    "            'val': val_ids,\n",
    "            'test': test_ids\n",
    "        }\n",
    "        \n",
    "        # Print split statistics\n",
    "        print(f\"\\n📈 Split Statistics:\")\n",
    "        for split_name, case_ids in splits.items():\n",
    "            split_df = valid_cases[valid_cases['case_id'].isin(case_ids)]\n",
    "            split_counts = split_df['size_class'].value_counts()\n",
    "            print(f\"{split_name.upper()}: {len(case_ids)} cases\")\n",
    "            for size_class in ['small', 'medium', 'large']:\n",
    "                count = split_counts.get(size_class, 0)\n",
    "                pct = (count / len(case_ids) * 100) if len(case_ids) > 0 else 0\n",
    "                print(f\"  {size_class}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def create_conditioning_features(self, metadata_df):\n",
    "        \"\"\"\n",
    "        Create conditioning features for CLDM training\n",
    "        \n",
    "        Args:\n",
    "            metadata_df: DataFrame with case metadata\n",
    "            \n",
    "        Returns:\n",
    "            Enhanced metadata with conditioning features\n",
    "        \"\"\"\n",
    "        # Add size class encoding\n",
    "        size_class_map = {'small': 0, 'medium': 1, 'large': 2}\n",
    "        metadata_df['size_class_id'] = metadata_df['size_class'].map(size_class_map)\n",
    "        \n",
    "        # Normalize volume for continuous conditioning\n",
    "        log_volumes = np.log1p(metadata_df['tumor_volume_mm3'])\n",
    "        metadata_df['volume_normalized'] = (log_volumes - log_volumes.mean()) / log_volumes.std()\n",
    "        \n",
    "        # Calculate compactness (based on your findings)\n",
    "        metadata_df['tumor_compactness'] = self._calculate_compactness_scores(metadata_df)\n",
    "        \n",
    "        # Add anatomical position encoding (pancreas head/body/tail)\n",
    "        metadata_df['anatomical_region'] = self._infer_anatomical_region(metadata_df)\n",
    "        \n",
    "        return metadata_df\n",
    "    \n",
    "    def _calculate_compactness_scores(self, metadata_df):\n",
    "        \"\"\"Calculate tumor compactness scores\"\"\"\n",
    "        # Simplified compactness based on bounding box analysis\n",
    "        # In practice, this would use the actual 3D shape analysis\n",
    "        compactness_scores = []\n",
    "        \n",
    "        for _, row in metadata_df.iterrows():\n",
    "            if row['size_class'] == 'large':\n",
    "                compactness = np.random.normal(0.056, 0.019)  # From your analysis\n",
    "            elif row['size_class'] == 'medium':\n",
    "                compactness = np.random.normal(0.104, 0.036)\n",
    "            else:  # small\n",
    "                compactness = np.random.normal(0.109, 0.035)\n",
    "            \n",
    "            compactness_scores.append(max(0.01, compactness))  # Ensure positive\n",
    "        \n",
    "        return compactness_scores\n",
    "    \n",
    "    def _infer_anatomical_region(self, metadata_df):\n",
    "        \"\"\"Infer anatomical region (head/body/tail) from centroid\"\"\"\n",
    "        regions = []\n",
    "        \n",
    "        for _, row in metadata_df.iterrows():\n",
    "            # Simplified region inference based on centroid Z position\n",
    "            # In practice, this would use more sophisticated anatomical landmarks\n",
    "            centroid = row['tumor_centroid']\n",
    "            z_pos = centroid[2] if isinstance(centroid, (list, tuple)) else 0\n",
    "            \n",
    "            if z_pos < 0.3:\n",
    "                regions.append('head')\n",
    "            elif z_pos < 0.7:\n",
    "                regions.append('body')\n",
    "            else:\n",
    "                regions.append('tail')\n",
    "        \n",
    "        return regions\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"\n",
    "        Execute the complete data processing pipeline\n",
    "        \n",
    "        Returns:\n",
    "            Processing summary and metadata DataFrame\n",
    "        \"\"\"\n",
    "        print(\"🚀 Starting Complete Pancreatic Tumor Data Pipeline\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load MSD dataset\n",
    "        train_files, dataset_info = self.load_msd_data()\n",
    "        \n",
    "        # Step 2: Process all cases\n",
    "        print(f\"\\n🔄 Processing {len(train_files)} cases...\")\n",
    "        metadata_list = []\n",
    "        \n",
    "        for i, case_info in enumerate(train_files):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Progress: {i}/{len(train_files)} cases processed\")\n",
    "            \n",
    "            metadata = self.process_single_case(case_info)\n",
    "            if metadata:\n",
    "                metadata_list.append(metadata)\n",
    "        \n",
    "        # Step 3: Create metadata DataFrame\n",
    "        metadata_df = pd.DataFrame(metadata_list)\n",
    "        successful_cases = metadata_df[metadata_df['processing_successful'] == True]\n",
    "        \n",
    "        print(f\"\\n📊 Processing Summary:\")\n",
    "        print(f\"Total cases processed: {len(metadata_list)}\")\n",
    "        print(f\"Successful: {len(successful_cases)}\")\n",
    "        print(f\"Failed: {len(metadata_list) - len(successful_cases)}\")\n",
    "        \n",
    "        if len(successful_cases) == 0:\n",
    "            print(\"❌ No cases processed successfully!\")\n",
    "            return None, None\n",
    "        \n",
    "        # Step 4: Add conditioning features\n",
    "        print(f\"\\n🎯 Creating conditioning features...\")\n",
    "        enhanced_metadata = self.create_conditioning_features(successful_cases)\n",
    "        \n",
    "        # Step 5: Create stratified splits\n",
    "        splits = self.create_stratified_splits(enhanced_metadata)\n",
    "        \n",
    "        # Step 6: Save everything\n",
    "        self.save_pipeline_outputs(enhanced_metadata, splits, dataset_info)\n",
    "        \n",
    "        # Step 7: Generate summary report\n",
    "        self.generate_summary_report(enhanced_metadata, splits)\n",
    "        \n",
    "        print(f\"\\n🎉 Pipeline completed successfully!\")\n",
    "        print(f\"📁 All outputs saved to: {self.output_dir}\")\n",
    "        \n",
    "        return enhanced_metadata, splits\n",
    "    \n",
    "    def save_pipeline_outputs(self, metadata_df, splits, dataset_info):\n",
    "        \"\"\"Save all pipeline outputs\"\"\"\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_df.to_csv(self.output_dir / 'metadata' / 'case_metadata.csv', index=False)\n",
    "        \n",
    "        # Save splits\n",
    "        for split_name, case_ids in splits.items():\n",
    "            with open(self.output_dir / 'splits' / f'{split_name}_cases.txt', 'w') as f:\n",
    "                f.write('\\n'.join(case_ids))\n",
    "        \n",
    "        # Save split metadata\n",
    "        splits_metadata = {}\n",
    "        for split_name, case_ids in splits.items():\n",
    "            split_df = metadata_df[metadata_df['case_id'].isin(case_ids)]\n",
    "            splits_metadata[split_name] = {\n",
    "                'case_count': len(case_ids),\n",
    "                'size_distribution': split_df['size_class'].value_counts().to_dict(),\n",
    "                'volume_stats': split_df.groupby('size_class')['tumor_volume_mm3'].agg(['count', 'mean', 'std']).to_dict()\n",
    "            }\n",
    "        \n",
    "        with open(self.output_dir / 'splits' / 'split_statistics.json', 'w') as f:\n",
    "            json.dump(splits_metadata, f, indent=2)\n",
    "        \n",
    "        # Save processing config\n",
    "        config = {\n",
    "            'target_spacing': self.target_spacing,\n",
    "            'target_size': self.target_size,\n",
    "            'hu_window': self.hu_window,\n",
    "            'size_thresholds': self.size_thresholds,\n",
    "            'dataset_info': dataset_info\n",
    "        }\n",
    "        \n",
    "        with open(self.output_dir / 'metadata' / 'pipeline_config.json', 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Saved metadata, splits, and configuration files\")\n",
    "    \n",
    "    def generate_summary_report(self, metadata_df, splits):\n",
    "        \"\"\"Generate comprehensive summary report\"\"\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"🏥 PANCREATIC TUMOR CLDM DATA PIPELINE REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_cases = len(metadata_df)\n",
    "        size_dist = metadata_df['size_class'].value_counts()\n",
    "        \n",
    "        report.append(f\"\\n📊 DATASET OVERVIEW\")\n",
    "        report.append(f\"Total processed cases: {total_cases}\")\n",
    "        report.append(f\"Size class distribution:\")\n",
    "        for size_class, count in size_dist.items():\n",
    "            pct = count / total_cases * 100\n",
    "            report.append(f\"  {size_class.capitalize()}: {count} cases ({pct:.1f}%)\")\n",
    "        \n",
    "        # Volume statistics\n",
    "        report.append(f\"\\n📈 VOLUME STATISTICS\")\n",
    "        volume_stats = metadata_df.groupby('size_class')['tumor_volume_mm3'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "        for size_class, stats in volume_stats.iterrows():\n",
    "            report.append(f\"{size_class.capitalize()} tumors:\")\n",
    "            report.append(f\"  Volume range: {stats['min']:.0f} - {stats['max']:.0f} mm³\")\n",
    "            report.append(f\"  Mean ± Std: {stats['mean']:.0f} ± {stats['std']:.0f} mm³\")\n",
    "        \n",
    "        # Split information\n",
    "        report.append(f\"\\n🔄 DATA SPLITS\")\n",
    "        for split_name, case_ids in splits.items():\n",
    "            split_df = metadata_df[metadata_df['case_id'].isin(case_ids)]\n",
    "            split_dist = split_df['size_class'].value_counts()\n",
    "            report.append(f\"{split_name.upper()} SET: {len(case_ids)} cases\")\n",
    "            for size_class in ['small', 'medium', 'large']:\n",
    "                count = split_dist.get(size_class, 0)\n",
    "                report.append(f\"  {size_class}: {count}\")\n",
    "        \n",
    "        # CLDM training recommendations\n",
    "        report.append(f\"\\n🧠 CLDM TRAINING RECOMMENDATIONS\")\n",
    "        report.append(f\"1. Use heavy augmentation for large tumors (only {size_dist.get('large', 0)} cases)\")\n",
    "        report.append(f\"2. Implement size-adaptive conditioning strength\")\n",
    "        report.append(f\"3. Progressive training: Small → Medium → Large\")\n",
    "        report.append(f\"4. Consider synthetic large tumor generation\")\n",
    "        \n",
    "        # Conditioning features\n",
    "        report.append(f\"\\n🎯 CONDITIONING FEATURES READY\")\n",
    "        report.append(f\"✅ Size class embeddings (0: small, 1: medium, 2: large)\")\n",
    "        report.append(f\"✅ Normalized tumor volumes (continuous)\")\n",
    "        report.append(f\"✅ Compactness scores (anatomical realism)\")\n",
    "        report.append(f\"✅ Anatomical region encoding (head/body/tail)\")\n",
    "        \n",
    "        # Save report\n",
    "        report_text = '\\n'.join(report)\n",
    "        with open(self.output_dir / 'metadata' / 'pipeline_report.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(report_text)\n",
    "\n",
    "        \n",
    "        print(report_text)\n",
    "        return report_text\n",
    "\n",
    "# Data Loader for CLDM Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86765f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PancreasCLDMDataset:\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for CLDM training\n",
    "    Loads processed volumes with size-based conditioning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, case_ids, data_dir, metadata_df, augment=False):\n",
    "        \"\"\"\n",
    "        Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            case_ids: List of case IDs for this split\n",
    "            data_dir: Directory with processed data\n",
    "            metadata_df: DataFrame with case metadata\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.case_ids = case_ids\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.metadata_df = metadata_df.set_index('case_id')\n",
    "        self.augment = augment\n",
    "        \n",
    "        print(f\"📦 Dataset initialized: {len(case_ids)} cases\")\n",
    "        \n",
    "        # Class weights for handling imbalance\n",
    "        size_counts = metadata_df[metadata_df['case_id'].isin(case_ids)]['size_class'].value_counts()\n",
    "        total = len(case_ids)\n",
    "        self.class_weights = {\n",
    "            'small': total / (3 * size_counts.get('small', 1)),\n",
    "            'medium': total / (3 * size_counts.get('medium', 1)),\n",
    "            'large': total / (3 * size_counts.get('large', 1))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single training sample\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with volume, masks, and conditioning information\n",
    "        \"\"\"\n",
    "        case_id = self.case_ids[idx]\n",
    "        metadata = self.metadata_df.loc[case_id]\n",
    "        \n",
    "        # Load processed data\n",
    "        volume = np.load(self.data_dir / 'processed_volumes' / f'{case_id}_volume.npy')\n",
    "        pancreas_mask = np.load(self.data_dir / 'processed_masks' / f'{case_id}_pancreas.npy')\n",
    "        tumor_mask = np.load(self.data_dir / 'processed_masks' / f'{case_id}_tumor.npy')\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment and np.random.random() > 0.5:\n",
    "            volume, pancreas_mask, tumor_mask = self._apply_augmentation(\n",
    "                volume, pancreas_mask, tumor_mask, metadata['size_class']\n",
    "            )\n",
    "        \n",
    "        # Prepare conditioning information\n",
    "        conditioning = {\n",
    "            'size_class_id': metadata['size_class_id'],\n",
    "            'size_class': metadata['size_class'],\n",
    "            'volume_normalized': metadata['volume_normalized'],\n",
    "            'tumor_volume_mm3': metadata['tumor_volume_mm3'],\n",
    "            'compactness': metadata['tumor_compactness'],\n",
    "            'anatomical_region': metadata['anatomical_region'],\n",
    "            'class_weight': self.class_weights[metadata['size_class']]\n",
    "        }\n",
    "        \n",
    "        # Create combined mask for visualization\n",
    "        combined_mask = pancreas_mask.astype(np.float32) + tumor_mask.astype(np.float32) * 2\n",
    "        \n",
    "        return {\n",
    "            'volume': volume.astype(np.float32),\n",
    "            'pancreas_mask': pancreas_mask.astype(np.float32),\n",
    "            'tumor_mask': tumor_mask.astype(np.float32),\n",
    "            'combined_mask': combined_mask,\n",
    "            'conditioning': conditioning,\n",
    "            'case_id': case_id\n",
    "        }\n",
    "    \n",
    "    def _apply_augmentation(self, volume, pancreas_mask, tumor_mask, size_class):\n",
    "        \"\"\"\n",
    "        Apply size-class specific augmentation\n",
    "        Heavy augmentation for rare large tumors\n",
    "        \"\"\"\n",
    "        # Intensity augmentation\n",
    "        if np.random.random() > 0.5:\n",
    "            # Slight intensity shift (within medical range)\n",
    "            intensity_shift = np.random.normal(0, 0.1)\n",
    "            volume = volume + intensity_shift\n",
    "        \n",
    "        # Spatial augmentation (more aggressive for rare classes)\n",
    "        augment_strength = {\n",
    "            'small': 0.1,   # Light augmentation\n",
    "            'medium': 0.2,  # Moderate augmentation  \n",
    "            'large': 0.4    # Heavy augmentation\n",
    "        }\n",
    "        \n",
    "        strength = augment_strength.get(size_class, 0.1)\n",
    "        \n",
    "        if np.random.random() > 0.5:\n",
    "            # Elastic deformation (simplified)\n",
    "            noise = np.random.normal(0, strength, volume.shape)\n",
    "            volume = volume + noise * 0.1\n",
    "        \n",
    "        return volume, pancreas_mask, tumor_mask\n",
    "\n",
    "# Utility Functions for Pipeline Management\n",
    "def setup_data_pipeline(data_root, output_dir, config=None):\n",
    "    \"\"\"\n",
    "    Setup and configure the complete data pipeline\n",
    "    \n",
    "    Args:\n",
    "        data_root: Path to MSD Task07_Pancreas dataset\n",
    "        output_dir: Output directory for processed data\n",
    "        config: Optional configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Configured PancreasDataPipeline instance\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'target_spacing': (1.0, 1.0, 1.0),\n",
    "            'target_size': (128, 128, 64),\n",
    "            'hu_window': (-100, 240)\n",
    "        }\n",
    "    \n",
    "    pipeline = PancreasDataPipeline(\n",
    "        data_root=data_root,\n",
    "        output_dir=output_dir,\n",
    "        target_spacing=config['target_spacing'],\n",
    "        target_size=config['target_size'],\n",
    "        hu_window=config['hu_window']\n",
    "    )\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def load_processed_data(output_dir):\n",
    "    \"\"\"\n",
    "    Load processed data and metadata\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory with processed pipeline outputs\n",
    "        \n",
    "    Returns:\n",
    "        metadata_df, splits, config\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_df = pd.read_csv(output_path / 'metadata' / 'case_metadata.csv')\n",
    "    \n",
    "    # Load splits\n",
    "    splits = {}\n",
    "    for split_file in (output_path / 'splits').glob('*_cases.txt'):\n",
    "        split_name = split_file.stem.replace('_cases', '')\n",
    "        with open(split_file, 'r') as f:\n",
    "            splits[split_name] = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # Load config\n",
    "    with open(output_path / 'metadata' / 'pipeline_config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    return metadata_df, splits, config\n",
    "\n",
    "def create_cldm_dataloaders(output_dir, batch_size=4, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders for CLDM training\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory with processed data\n",
    "        batch_size: Batch size for training\n",
    "        num_workers: Number of workers for data loading\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with train/val/test DataLoaders\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        from torch.utils.data import DataLoader\n",
    "    except ImportError:\n",
    "        print(\"❌ PyTorch not available. Install with: pip install torch\")\n",
    "        return None\n",
    "    \n",
    "    # Load processed data\n",
    "    metadata_df, splits, config = load_processed_data(output_dir)\n",
    "    \n",
    "    # Create datasets\n",
    "    datasets = {}\n",
    "    for split_name, case_ids in splits.items():\n",
    "        augment = (split_name == 'train')  # Only augment training data\n",
    "        datasets[split_name] = PancreasCLDMDataset(\n",
    "            case_ids=case_ids,\n",
    "            data_dir=output_dir,\n",
    "            metadata_df=metadata_df,\n",
    "            augment=augment\n",
    "        )\n",
    "    \n",
    "    # Create data loaders\n",
    "    dataloaders = {}\n",
    "    for split_name, dataset in datasets.items():\n",
    "        shuffle = (split_name == 'train')\n",
    "        dataloaders[split_name] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    print(f\"🔄 Created DataLoaders:\")\n",
    "    for split_name, loader in dataloaders.items():\n",
    "        print(f\"  {split_name}: {len(loader.dataset)} samples, {len(loader)} batches\")\n",
    "    \n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191201de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pipeline_example():\n",
    "    \"\"\"\n",
    "    Example of how to run the complete pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATA_ROOT = \"path/to/Task07_Pancreas\"  # Replace with your path\n",
    "    OUTPUT_DIR = \"processed_pancreas_data\"\n",
    "    \n",
    "    # Pipeline configuration\n",
    "    config = {\n",
    "        'target_spacing': (1.0, 1.0, 1.0),      # 1mm isotropic\n",
    "        'target_size': (128, 128, 64),           # Standard size for diffusion\n",
    "        'hu_window': (-100, 240)                 # Pancreas-optimized HU window\n",
    "    }\n",
    "    \n",
    "    print(\"🚀 PANCREATIC TUMOR CLDM PIPELINE EXAMPLE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Setup pipeline\n",
    "    pipeline = setup_data_pipeline(DATA_ROOT, OUTPUT_DIR, config)\n",
    "    \n",
    "    # Step 2: Run complete processing\n",
    "    metadata_df, splits = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    if metadata_df is not None:\n",
    "        # Step 3: Create data loaders for CLDM training\n",
    "        dataloaders = create_cldm_dataloaders(OUTPUT_DIR, batch_size=2)\n",
    "        \n",
    "        # Step 4: Test data loading\n",
    "        if dataloaders:\n",
    "            print(\"\\n🧪 Testing data loading...\")\n",
    "            train_loader = dataloaders['train']\n",
    "            sample_batch = next(iter(train_loader))\n",
    "            \n",
    "            print(f\"Sample batch shapes:\")\n",
    "            print(f\"  Volume: {sample_batch['volume'].shape}\")\n",
    "            print(f\"  Pancreas mask: {sample_batch['pancreas_mask'].shape}\")\n",
    "            print(f\"  Tumor mask: {sample_batch['tumor_mask'].shape}\")\n",
    "            print(f\"  Conditioning keys: {list(sample_batch['conditioning'].keys())}\")\n",
    "            \n",
    "            # Display conditioning info for first sample\n",
    "            cond = sample_batch['conditioning']\n",
    "            print(f\"\\n🎯 Sample conditioning information:\")\n",
    "            print(f\"  Size class: {cond['size_class'][0]} (ID: {cond['size_class_id'][0]})\")\n",
    "            print(f\"  Tumor volume: {cond['tumor_volume_mm3'][0]:.0f} mm³\")\n",
    "            print(f\"  Normalized volume: {cond['volume_normalized'][0]:.3f}\")\n",
    "            print(f\"  Compactness: {cond['compactness'][0]:.3f}\")\n",
    "            print(f\"  Anatomical region: {cond['anatomical_region'][0]}\")\n",
    "    \n",
    "    print(\"\\n✅ Pipeline setup complete! Ready for CLDM training.\")\n",
    "    return pipeline, metadata_df, splits\n",
    "\n",
    "# Quality Control and Validation\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_processed_data(output_dir, num_samples=5):\n",
    "    \"\"\"\n",
    "    Validate processed data quality\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory with processed data\n",
    "        num_samples: Number of random samples to validate\n",
    "    \"\"\"\n",
    "    metadata_df, splits, config = load_processed_data(output_dir)\n",
    "    \n",
    "    print(\"🔍 DATA QUALITY VALIDATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Random sample validation\n",
    "    sample_cases = metadata_df.sample(min(num_samples, len(metadata_df)))\n",
    "    \n",
    "    for _, case in sample_cases.iterrows():\n",
    "        case_id = case['case_id']\n",
    "        \n",
    "        try:\n",
    "            # Load processed files\n",
    "            volume = np.load(Path(output_dir) / 'processed_volumes' / f'{case_id}_volume.npy')\n",
    "            pancreas = np.load(Path(output_dir) / 'processed_masks' / f'{case_id}_pancreas.npy')\n",
    "            tumor = np.load(Path(output_dir) / 'processed_masks' / f'{case_id}_tumor.npy')\n",
    "            \n",
    "            # Validate shapes\n",
    "            assert volume.shape == tuple(config['target_size']), f\"Volume shape mismatch: {volume.shape}\"\n",
    "            assert pancreas.shape == tuple(config['target_size']), f\"Pancreas shape mismatch: {pancreas.shape}\"\n",
    "            assert tumor.shape == tuple(config['target_size']), f\"Tumor shape mismatch: {tumor.shape}\"\n",
    "            \n",
    "            # Validate data ranges\n",
    "            assert np.isfinite(volume).all(), f\"Volume contains invalid values\"\n",
    "            assert set(np.unique(pancreas)).issubset({0, 1}), f\"Pancreas mask not binary\"\n",
    "            assert set(np.unique(tumor)).issubset({0, 1}), f\"Tumor mask not binary\"\n",
    "            \n",
    "            # Validate tumor is within pancreas (mostly)\n",
    "            tumor_in_pancreas = np.sum((tumor == 1) & (pancreas == 1))\n",
    "            tumor_total = np.sum(tumor == 1)\n",
    "            if tumor_total > 0:\n",
    "                overlap_ratio = tumor_in_pancreas / tumor_total\n",
    "                if overlap_ratio < 0.7:  # Allow some margin for resampling artifacts\n",
    "                    print(f\"⚠️  {case_id}: Low tumor-pancreas overlap ({overlap_ratio:.2f})\")\n",
    "            \n",
    "            print(f\" {case_id}: {case['size_class']} tumor, {case['tumor_volume_mm3']:.0f} mm³\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" {case_id}: Validation failed - {str(e)}\")\n",
    "    \n",
    "    print(\"\\nQuality validation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Pancreatic Tumor CLDM Data Pipeline\")\n",
    "    print(\"Ready to process MSD Task07_Pancreas dataset\")\n",
    "    print(\"\\nTo use this pipeline:\")\n",
    "    print(\"1. Update DATA_ROOT path to your MSD dataset location\")\n",
    "    print(\"2. Run: pipeline, metadata, splits = run_pipeline_example()\")\n",
    "    print(\"3. Use create_cldm_dataloaders() for PyTorch training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"F:\\Conditional Latent Diffusion Model for Pancreas\\Task07_Pancreas\"\n",
    "OUTPUT_DIR = \"processed_pancreas_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = setup_data_pipeline(DATA_ROOT, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df, splits = pipeline.run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_cldm_dataloaders(OUTPUT_DIR, batch_size=4)\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "for batch in train_loader:\n",
    "    volume = batch['volume']           \n",
    "    conditioning = batch['conditioning'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71673ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_processed_data(OUTPUT_DIR, num_samples=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
